# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zro5W4jW35dkZxI38gDvZGGZVSh6Wohw
"""

!pip install pandas numpy scikit-learn matplotlib seaborn nltk


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

url = 'https://raw.githubusercontent.com/YBI-Foundation/Dataset/main/Financial%20Market%20News.csv'
df = pd.read_csv(url, encoding="ISO-8859-1")
df.head()

# Basic data information
df.info()

# Check for any missing values
print("Missing values in each column:")
print(df.isnull().sum())

# Summary statistics
df.describe()

# Plot the distribution of labels
sns.countplot(x='Label', data=df)
plt.title('Distribution of Sentiment Labels')
plt.show()

# Download NLTK stopwords and tokenizer
nltk.download('stopwords')
nltk.download('punkt')

# Preprocess the text: clean, tokenize, and remove stopwords
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = re.sub(r'[^\w\s]', '', text.lower())  # Remove punctuation and lowercase
    tokens = word_tokenize(text)  # Tokenize
    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
    return ' '.join(tokens)

# Apply preprocessing to all news columns
news_columns = ['News 1', 'News 2', 'News 3', 'News 4', 'News 5', 'News 6', 'News 7', 'News 8', 'News 9', 'News 10', 'News 11']
df['clean_text'] = df[news_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)
df['clean_text'] = df['clean_text'].apply(preprocess_text)

df.head()  # Display the dataframe after cleaning the text

# Define the target variable and feature variables
X = df['clean_text']  # Features (the combined and cleaned news headlines)
y = df['Label']  # Target (sentiment label)

# Display the shapes of X and y
print(f"Shape of X: {X.shape}")
print(f"Shape of y: {y.shape}")

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the shapes of training and testing sets
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")

# Vectorize the text using TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

# Convert the text data to TF-IDF features
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Train a Logistic Regression model
model = LogisticRegression()
model.fit(X_train_tfidf, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_tfidf)

# Evaluate the model's performance
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(classification_report(y_test, y_pred))

# Plot confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Example prediction using a new piece of news
new_news = ["The market rallies after positive earnings reports"]
new_news_cleaned = preprocess_text(new_news[0])
new_news_tfidf = tfidf_vectorizer.transform([new_news_cleaned])
predicted_label = model.predict(new_news_tfidf)

print(f"Predicted Sentiment: {predicted_label[0]}")